{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07c2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import json\n",
    "import csv\n",
    "import networkx as nx\n",
    "\n",
    "#Ease of Life libraries used to simulate progression of slow algorithms\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0203f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 967552 entries, 0 to 967551\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      967552 non-null  object \n",
      " 1   business_id  967552 non-null  object \n",
      " 2   stars        967552 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 22.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Load the data as given.\n",
    "data = pd.read_csv(\"philly_users_businesses_stars.csv\", encoding=\"latin\")\n",
    "\n",
    "print(data.info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456929a3",
   "metadata": {},
   "source": [
    "# Part 1: Aggressive Pruning\n",
    "Firstly, we prune our data as needed, using a modified version of our code from the previous exercise set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839f038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Data Size:  967552\n",
      "Current Data Size:  167056\n",
      "Current Data Size:  117066\n",
      "Current Data Size:  106727\n",
      "Current Data Size:  103946\n",
      "Current Data Size:  103070\n",
      "Current Data Size:  102852\n",
      "Current Data Size:  102472\n",
      "Current Data Size:  102093\n",
      "Current Data Size:  101923\n",
      "\n",
      "Total Unique Users:  1604\n",
      "Total Unique Businesses:  887\n",
      "Total Size:  101923\n"
     ]
    }
   ],
   "source": [
    "#STEP 1 (Same as STEP 1 of previous exercise 2.)\n",
    "prevLen = -1\n",
    "currentLen = len(data)\n",
    "newData = data.copy()\n",
    "\n",
    "#Iterate the copied data. If after the iteration the size is the same, stop. otherwise, repeat.\n",
    "while prevLen != currentLen:\n",
    "    print(\"Current Data Size: \", len(newData))\n",
    "    \n",
    "    #Prune Non-Active Users.\n",
    "    activeUsers = newData.groupby([\"user_id\"]).count()\n",
    "    activeUsers = activeUsers.loc[activeUsers[\"business_id\"] > 30].reset_index()\n",
    "    newData = newData.loc[data[\"user_id\"].isin(activeUsers[\"user_id\"].unique())]\n",
    "\n",
    "    #Prune Non-Active Businesses.\n",
    "    activeBusinesses = newData.groupby([\"business_id\"]).count()\n",
    "    activeBusinesses = activeBusinesses.loc[activeBusinesses[\"user_id\"] > 50].reset_index()\n",
    "    newData = newData.loc[data[\"business_id\"].isin(activeBusinesses[\"business_id\"].unique())]\n",
    "    \n",
    "    prevLen = currentLen\n",
    "    currentLen = len(newData)\n",
    "print(\"\\nTotal Unique Users: \", len(newData[\"user_id\"].unique()))\n",
    "print(\"Total Unique Businesses: \", len(newData[\"business_id\"].unique()))\n",
    "print(\"Total Size: \", len(newData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ef38a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the data by chunk, finding the important data needed.\n",
    "#<!-- Run this only once! We store the nodes for future iterations in filtered_users --!>\n",
    "exit(1) #REMOVE TO RUN\n",
    "chunkFrame = pd.DataFrame()\n",
    "with pd.read_json(\"yelp_academic_dataset_user.json\", lines=True, encoding=\"latin\", chunksize=100_000) as file:\n",
    "    for chunk in file:\n",
    "        chunk = chunk[[\"user_id\", \"friends\"]]\n",
    "        chunk = chunk.loc[chunk[\"user_id\"].isin(newData[\"user_id\"])]\n",
    "        user_df.append(chunk)\n",
    "user_df.to_csv(\"filtered_users.csv\", index=False, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a84a33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1583 entries, 0 to 1603\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   user_id  1583 non-null   object\n",
      " 1   friends  1583 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 37.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Load the filtered users\n",
    "userData = pd.read_csv(\"filtered_users.csv\", encoding=\"latin\")\n",
    "#Turn the friends into an array\n",
    "userData[\"friends\"] = userData[\"friends\"].apply(lambda x : x.split(\", \"))\n",
    "#Remove those without friends\n",
    "userData = userData.loc[~userData.index.isin(userData[userData.apply(lambda x : True if \"None\" in x.friends else False, axis=1)].index)]\n",
    "reviewData = newData.copy()\n",
    "print(userData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b0dce",
   "metadata": {},
   "source": [
    "# Part 1: The Graph\n",
    "Having our data loaded, we find all users that we need, as well as their friendships within the same set of users. We get 1604 users, from which after finding the largest strongly connected component and isolating it, we end up with 1504 unique users, 887 businesses, and 19347 friendships amongst the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c49d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 1504 \n",
      "Frienships: 19347 \n",
      "Businesses: 887\n"
     ]
    }
   ],
   "source": [
    "#Part 1 - Forge the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "importantIDs = userData[\"user_id\"].to_list()\n",
    "#Initialize nodes\n",
    "for uid in userData[\"user_id\"]:\n",
    "    G.add_node(uid)\n",
    "#Connect edges\n",
    "for uid, friends in zip(userData[\"user_id\"], userData[\"friends\"]):\n",
    "    for f in friends:\n",
    "        if f in importantIDs:\n",
    "            G.add_edge(uid, f)\n",
    "            \n",
    "#Find the strongly connected components and remove the nodes not in them.\n",
    "stronglyConnectedNodes = max(nx.connected_components(G), key=len)\n",
    "removedNodes = [i for i in G.nodes if i not in stronglyConnectedNodes]\n",
    "for node in removedNodes:\n",
    "    G.remove_node(node)\n",
    "    \n",
    "#Prune unwanted users               \n",
    "reviewData = reviewData.loc[reviewData[\"user_id\"].isin(G.nodes())]\n",
    "print(\"Users:\", len(G.nodes), \"\\nFrienships:\", len(G.edges), \"\\nBusinesses:\", len(reviewData[\"business_id\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242595e",
   "metadata": {},
   "source": [
    "# Part 2: Train And Test Data\n",
    "A simple seperation of our data to sample them into train and test will suffice for this step. However, as requested, we instead load the pre-sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f737bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "sampleUsers = random.sample(reviewData[\"user_id\"].unique().tolist(), 100)\n",
    "sampleBusinesses = random.sample(reviewData[\"business_id\"].unique().tolist(), 100)\n",
    "\n",
    "testData = reviewData.loc[reviewData[\"user_id\"].isin(sampleUsers) & reviewData[\"business_id\"].isin(sampleBusinesses)]\n",
    "trainData = reviewData.loc[~reviewData.index.isin(testData.index)]\n",
    "\n",
    "testData = pd.read_csv(\"test_data.csv\", encoding=\"latin\").drop(\"Unnamed: 0\", axis=1).drop_duplicates()\n",
    "trainData = pd.read_csv(\"train_data.csv\", encoding=\"latin\").drop(\"Unnamed: 0\", axis=1).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131e3ee",
   "metadata": {},
   "source": [
    "# Part 3: Value Propagation\n",
    "A somewhat similar approach as the PageRank algorithm. Before we start, we do some preproccessing on our graph:\n",
    "All nodes get 2 attributes:\n",
    "- \"absorbing_bids\" contains a list of all the business IDs that this user has in our train data. Therefore, we have a linear way of finding whether the pair U,B is absorbing.\n",
    "- \"r\" contains a dictionary in the form of BID:R_VALUE. This represents the guess R_VALUE for the pair UID,BID where UID is the current node we are on. If no R value has been set, we have no entry on this dictionary.\n",
    "\n",
    "The algorithm then begins as expected. For each node and for each business, we check if the pair of node and business is absorbant. If it is, we continue, otherwise, we calculate its new R_VALUE using its neighbors. All neighbors with no R_VALUE for that pair are just ignored. This continious until we see changes bellow our threshold (10^-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa790bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3\n",
    "#Test BIDs\n",
    "testBusinessIds = testData[\"business_id\"].unique()\n",
    "\n",
    "#A listed group-by dataframe of all UIDs in our train data.\n",
    "absorbingPairsBids = trainData.groupby([\"user_id\"])[\"business_id\"].apply(list).reset_index()\n",
    "absorbingPairsStars = trainData.groupby([\"user_id\"])[\"stars\"].apply(list).reset_index()\n",
    "absorbingPairs = absorbingPairsBids.merge(absorbingPairsStars, right_on = [\"user_id\"], left_on = [\"user_id\"])\n",
    "\n",
    "#Mark absorbant/not-absorbant\n",
    "graph = G.copy()\n",
    "for node in graph.nodes():\n",
    "    bids = absorbingPairs[absorbingPairs[\"user_id\"] == node]\n",
    "    \n",
    "    #Non Absorbant\n",
    "    if len(bids) == 0:\n",
    "        graph.nodes[node][\"absorbing_bids\"] = []\n",
    "        graph.nodes[node][\"r\"] = {}\n",
    "    else: #Absorbant\n",
    "        graph.nodes[node][\"absorbing_bids\"] = bids.iloc[0][\"business_id\"]\n",
    "        firstSample = bids.iloc[0]\n",
    "        graph.nodes[node][\"r\"] = {firstSample[\"business_id\"][i]:firstSample[\"stars\"][i] for i in range(len(firstSample[\"stars\"]))}\n",
    "    \n",
    "\n",
    "#absorbing_bids: All BIDs of this node that have a pair in the Train Data\n",
    "#r: The R values. Default = null. If absorbant: A Dictionary matching Bid:Stars for the static values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb4f7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  25\n",
      "Iteration  50\n",
      "Iteration  75\n",
      "Iteration  100\n",
      "Iteration  125\n",
      "Iteration  150\n",
      "Iteration  175\n",
      "Iteration  200\n"
     ]
    }
   ],
   "source": [
    "hasBigChange = True\n",
    "counter = 0\n",
    "\n",
    "while (hasBigChange):\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 25 == 0:\n",
    "        print(\"Iteration \", counter)\n",
    "        \n",
    "    hasBigChange = False\n",
    "    for node in graph.nodes():\n",
    "        for bid in testBusinessIds:\n",
    "            bids = graph.nodes[node][\"absorbing_bids\"]\n",
    "            isAbsorbant = True if bid in bids else False\n",
    "\n",
    "            if isAbsorbant:\n",
    "                continue\n",
    "\n",
    "            R = 0\n",
    "            for n in graph.neighbors(node):\n",
    "                if bid in graph.nodes[n][\"r\"]:\n",
    "                    R += graph.nodes[n][\"r\"][bid]\n",
    "            R = R/graph.degree(node)\n",
    "\n",
    "            prevR = R+1\n",
    "            if bid in graph.nodes[node][\"r\"]:\n",
    "                prevR = graph.nodes[node][\"r\"][bid]\n",
    "            graph.nodes[node][\"r\"][bid] = R\n",
    "\n",
    "            difference = abs(abs(prevR) - abs(R))\n",
    "            if difference > 0.000001: #10^-6\n",
    "                hasBigChange = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80978624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8988345631400708"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#30m - 0.8988347888436959 - 10^-6\n",
    "#13m - 0.898927860603693 - 10^-3\n",
    "\n",
    "#5m - 0.8987000999129854 - 10^-3 ++\n",
    "#16m - 0.8988345631400708 - 10^-6 ++\n",
    "testingData = testData.copy()\n",
    "testingData[\"predict\"] = testingData.apply(lambda x : graph.nodes[x.user_id][\"r\"][x.business_id], axis=1)\n",
    "RMSE = sklearn.metrics.mean_squared_error(testingData[\"stars\"], testingData[\"predict\"], squared=False)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a77879",
   "metadata": {},
   "source": [
    "# Value Propagation - Improved\n",
    "The algorithm takes a fair bit of time (~25m), so as an improvement, we add another attribute to all nodes.\n",
    "-\"isAbsorbing\" dictates as a boolean whether this node is absorbing for the business we are checking.\n",
    "\n",
    "For this to work, we change the order of our loops, and instead calculate the guess for each business seperatly. This way, we only check whether a node is absorbing for that bussiness on the first iteration, and on next iterations we simply know. This improves the iteration time to ~18m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae0ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190a98702b724e908fe789504d64ae80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Improved to O(N) complexity for absorbant validation\n",
    "loadingBar = IntProgress(min=0, max=len(testBusinessIds)) # instantiate the bar\n",
    "display(loadingBar)\n",
    "\n",
    "for bid in testBusinessIds:\n",
    "    \n",
    "    loadingBar.value += 1\n",
    "    hasBigChange = True\n",
    "    firstIteration = True\n",
    "    \n",
    "    while(hasBigChange):\n",
    "        hasBigChange = False\n",
    "        \n",
    "        for node in graph.nodes():\n",
    "            if firstIteration:\n",
    "                bids = graph.nodes[node][\"absorbing_bids\"]\n",
    "                isAbsorbant = True if bid in bids else False\n",
    "                graph.nodes[node][\"isAbsorbant\"] = isAbsorbant\n",
    "            else:\n",
    "                isAbsorbant = graph.nodes[node][\"isAbsorbant\"]\n",
    "            \n",
    "            if isAbsorbant:\n",
    "                continue\n",
    "                \n",
    "            R = 0\n",
    "            for n in graph.neighbors(node):\n",
    "                if bid in graph.nodes[n][\"r\"]:\n",
    "                    R += graph.nodes[n][\"r\"][bid]\n",
    "            R = R/graph.degree(node)\n",
    "\n",
    "            prevR = R+1\n",
    "            if bid in graph.nodes[node][\"r\"]:\n",
    "                prevR = graph.nodes[node][\"r\"][bid]\n",
    "            graph.nodes[node][\"r\"][bid] = R\n",
    "\n",
    "            difference = abs(abs(prevR) - abs(R))\n",
    "            if difference > 0.000001: #10^-6\n",
    "                hasBigChange = True\n",
    "        firstIteration = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15863da2",
   "metadata": {},
   "source": [
    "# Part 4: Conclusions\n",
    "\n",
    "| Algorithm   | RMSE  | Time  |\n",
    "|:----------:|:------:|:------:|\n",
    "| BA          | 0.867  |   <1s    |\n",
    "| UCF         | 0.892, K=40| ~1m  |\n",
    "| Value Prop  | 0.898  |   ~18m    |\n",
    "| UA          | 0.902  |    <1s   |\n",
    "| ICF         | 0.906  |   ~2m    |\n",
    "| SVD         | 3.61, K=100|  ~1m |\n",
    "\n",
    "By trying all algorithms, we still get the Business Average as the most efficient time and error wise algorithm. The Value Propagation algorithm manages to surpass almost all other algorithms in terms of RMSE, however time is its largest downside. With the hope the VP can be further optimised, which I haven't managed to do, I can see it climbing higher the ranking ladder in terms of time. However the error will hardly be reduced by reducing the threshold further.\n",
    "\n",
    "**Note: The algorithms where run using the code from exercise 2-2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16200371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
