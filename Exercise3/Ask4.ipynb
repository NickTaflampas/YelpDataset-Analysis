{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8d2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.cluster\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.datasets import make_blobs\n",
    "import sklearn.linear_model\n",
    "\n",
    "#Ease of Life libraries used to simulate progression of slow algorithms\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c49716",
   "metadata": {},
   "source": [
    "# Introductions: The Problem\n",
    "As stated, the problem we are facing is the construction of 2 dictionaries (or rather group of words) that hold either negative or positive words. Through out this Notebook, i will describe my thought process and the 3 attempts I constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f282b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 893906 entries, 0 to 893905\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   review_id    893906 non-null  object \n",
      " 1   user_id      893906 non-null  object \n",
      " 2   business_id  893906 non-null  object \n",
      " 3   stars        893906 non-null  float64\n",
      " 4   useful       893906 non-null  int64  \n",
      " 5   funny        893906 non-null  int64  \n",
      " 6   cool         893906 non-null  int64  \n",
      " 7   text         893906 non-null  object \n",
      " 8   date         893906 non-null  object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 68.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Filtered Reviews is a file from Exercise 1 that contains the Philadelphian reviews with non-empty values.\n",
    "reviews = pd.read_csv(\"filtered_reviews.csv\", encoding=\"latin\").dropna()\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c3c49",
   "metadata": {},
   "source": [
    "# Attempt 1: Highest Value\n",
    "To start things off, I considered this: Since we already know which review is \"Positive\" and \"Negative\", I can tag them without needing a classifier. But what is considered \"Positive\"? From a mathematical standpoint, I considered all reviews of 4 or 5 stars to be Positive, while 1 or 2 stared reviews are considered Negative. Reviews that have 3 stars are considered \"Neutral\" and thus worthless, thus removed.\n",
    "\n",
    "After training a TF-IDF vectorizer, we can get the vector for the POSITIVE reviews and the NEGATIVE reviews. The highest valued words should produce negative and positive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a80434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim neutral reviews\n",
    "data = reviews[~reviews[\"stars\"].isin([3])].copy()\n",
    "#Mark reviews with P for Positive (4-5 stars) and N for Negative (1-2 stars)\n",
    "data[\"category\"] = data[\"stars\"].apply(lambda x : \"P\" if x > 3 else \"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437c809a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_df=0.5, max_features=3000, min_df=0.0, stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.5, max_features=3000, min_df=0.0, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_df=0.5, max_features=3000, min_df=0.0, stop_words='english')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=3_000, stop_words=\"english\",\n",
    "                                                             min_df=0.0, max_df=0.5)\n",
    "documents = data.groupby([\"business_id\"])[\"text\"].sum().to_frame()\n",
    "vectorizer.fit(documents[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87052224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cluster P top words: delicious, menu, restaurant, chicken, cheese, bar, pizza, beer, sauce, dinner\n",
      "For Cluster N top words: restaurant, bar, table, cheese, pizza, chicken, car, menu, fries, manager\n"
     ]
    }
   ],
   "source": [
    "#Attempt 1\n",
    "evaluationData = data.sample(n=10000)\n",
    "\n",
    "inv_dic = {j:i for i,j in vectorizer.vocabulary_.items()}\n",
    "\n",
    "for clusterType in [\"P\", \"N\"]:\n",
    "    clusterReviews = evaluationData.loc[evaluationData[\"category\"] == clusterType]\n",
    "    text = clusterReviews[\"text\"].sum()\n",
    "    \n",
    "    vector = vectorizer.transform([text])\n",
    "    bestScores = np.argsort(vector.toarray()[0])\n",
    "    bestScores = np.flip(bestScores[-10:len(bestScores)])\n",
    "    \n",
    "    finalPrint = \"For Cluster \" + clusterType + \" top words: \"\n",
    "    for i in bestScores:\n",
    "        finalPrint += inv_dic[i] + \", \"\n",
    "    print(finalPrint[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5348ae2",
   "metadata": {},
   "source": [
    "# Results\n",
    "The first attempt came to be fruitless. First off, the vectorizer took a good while to train. Furthermore, the two categories did not produce any significant results. This could mean several things:\n",
    "- Perhaps each review contained several noise words, and taking them one-by-one would increase the chance of noise words such as \"Menu\", \"Restaurant\" or food related words which both belong on both groups\n",
    "- Perhaps the classification of \"Positive/Negative\" is incorrect\n",
    "- Or perhaps the TF-IDF method of highest value was incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85971257",
   "metadata": {},
   "source": [
    "# Attempt 2: Grouped Reviews\n",
    "We now try a slightly different approach:\n",
    "To avoid the first bullet point, this time we group up and train each review by business. This way, each business has its own vector, and we hope to reduce the \"restaurant stop words\".\n",
    "\n",
    "For the second bullet point, we consider the 3 Stared reviews. But we consider them as both positive and negative. The idea is that they perhaps contain information neccecary for our clasification.\n",
    "\n",
    "Lastly, we still use the TF-IDF value, but this time, each word is classified as Positive or Negative by the sum of its category score.\n",
    "\n",
    "This time, we make two groups: Positive and Negative businesses, which are seperated by their average review score (rounded). Afterwards, for each feature of our vectorizer, we calculate the sum of this feature's TF-IDF value for both categories. Whichever has the higher value (therefore the most frequent appearance) must belong to that group!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb0ecd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--OS_I7dnABrXvRCCuWOGQ</th>\n",
       "      <td>I have to say Len's auto body is the WORST. I ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03867995...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0M0b-XhtFagyLmsBtOe8w</th>\n",
       "      <td>REVIEW OF PARIS FLEA MARKET: Accidentally popp...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.03978147804701085, 0.0, 0.0, 0.0, 0.0122694...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0PN_KFPtbnLQZEeb23XiA</th>\n",
       "      <td>While there didn't seem to be anything wrong w...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0TffRSXXIlBYVbb5AwfTg</th>\n",
       "      <td>We went for my husbands birthday in a fairly l...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0027283746590056526, 0.011635157099922692, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0eUa8TsXFFy0FCxHYmrjg</th>\n",
       "      <td>Sandwiches had good taste and were a good size...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.009885542095808109, 0.0, 0.01519419388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zy_g2wKTNIB7EQdG73_Xaw</th>\n",
       "      <td>Einstein medical center in North Philadelphia ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.008139410954805908, 0.021544465692502488, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyghhZzPgb1bRAIYB-oi1w</th>\n",
       "      <td>I haven't written a review in years, but I fee...</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.0, 0.023168742201064813, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zz-fcqurtm77bZ_rVvo2Lw</th>\n",
       "      <td>Best lunch truck on campus hands down.  I've m...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.014573003981617209, 0.0, 0.02239887766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zz3E7kmJI2r2JseE6LAnrw</th>\n",
       "      <td>This is your typical Asian super market. I've ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.003403086645432603, 0.03377901847945415, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzyx5x0Z7xXWWvWnZFuxlQ</th>\n",
       "      <td>Maybe the pizza is good here... but I can real...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.030386888329486775, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11070 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "business_id                                                                 \n",
       "--OS_I7dnABrXvRCCuWOGQ  I have to say Len's auto body is the WORST. I ...   \n",
       "-0M0b-XhtFagyLmsBtOe8w  REVIEW OF PARIS FLEA MARKET: Accidentally popp...   \n",
       "-0PN_KFPtbnLQZEeb23XiA  While there didn't seem to be anything wrong w...   \n",
       "-0TffRSXXIlBYVbb5AwfTg  We went for my husbands birthday in a fairly l...   \n",
       "-0eUa8TsXFFy0FCxHYmrjg  Sandwiches had good taste and were a good size...   \n",
       "...                                                                   ...   \n",
       "zy_g2wKTNIB7EQdG73_Xaw  Einstein medical center in North Philadelphia ...   \n",
       "zyghhZzPgb1bRAIYB-oi1w  I haven't written a review in years, but I fee...   \n",
       "zz-fcqurtm77bZ_rVvo2Lw  Best lunch truck on campus hands down.  I've m...   \n",
       "zz3E7kmJI2r2JseE6LAnrw  This is your typical Asian super market. I've ...   \n",
       "zzyx5x0Z7xXWWvWnZFuxlQ  Maybe the pizza is good here... but I can real...   \n",
       "\n",
       "                        stars  \\\n",
       "business_id                     \n",
       "--OS_I7dnABrXvRCCuWOGQ      4   \n",
       "-0M0b-XhtFagyLmsBtOe8w      4   \n",
       "-0PN_KFPtbnLQZEeb23XiA      3   \n",
       "-0TffRSXXIlBYVbb5AwfTg      4   \n",
       "-0eUa8TsXFFy0FCxHYmrjg      4   \n",
       "...                       ...   \n",
       "zy_g2wKTNIB7EQdG73_Xaw      2   \n",
       "zyghhZzPgb1bRAIYB-oi1w      5   \n",
       "zz-fcqurtm77bZ_rVvo2Lw      4   \n",
       "zz3E7kmJI2r2JseE6LAnrw      4   \n",
       "zzyx5x0Z7xXWWvWnZFuxlQ      2   \n",
       "\n",
       "                                                                   vector  \n",
       "business_id                                                                \n",
       "--OS_I7dnABrXvRCCuWOGQ  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03867995...  \n",
       "-0M0b-XhtFagyLmsBtOe8w  [0.03978147804701085, 0.0, 0.0, 0.0, 0.0122694...  \n",
       "-0PN_KFPtbnLQZEeb23XiA  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "-0TffRSXXIlBYVbb5AwfTg  [0.0027283746590056526, 0.011635157099922692, ...  \n",
       "-0eUa8TsXFFy0FCxHYmrjg  [0.0, 0.009885542095808109, 0.0, 0.01519419388...  \n",
       "...                                                                   ...  \n",
       "zy_g2wKTNIB7EQdG73_Xaw  [0.008139410954805908, 0.021544465692502488, 0...  \n",
       "zyghhZzPgb1bRAIYB-oi1w  [0.0, 0.023168742201064813, 0.0, 0.0, 0.0, 0.0...  \n",
       "zz-fcqurtm77bZ_rVvo2Lw  [0.0, 0.014573003981617209, 0.0, 0.02239887766...  \n",
       "zz3E7kmJI2r2JseE6LAnrw  [0.003403086645432603, 0.03377901847945415, 0....  \n",
       "zzyx5x0Z7xXWWvWnZFuxlQ  [0.0, 0.0, 0.0, 0.0, 0.030386888329486775, 0.0...  \n",
       "\n",
       "[11070 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reviews.copy()\n",
    "evalData = data.groupby([\"business_id\"])[\"text\"].sum().to_frame()\n",
    "evalData[\"stars\"] = data.groupby([\"business_id\"])[\"stars\"].mean()\n",
    "evalData[\"stars\"] = evalData[\"stars\"].apply(lambda x : round(x))\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=3_000, stop_words=\"english\")\n",
    "matrix = vectorizer.fit_transform(evalData[\"text\"]).toarray()\n",
    "evalData[\"vector\"] = [matrix[i].tolist() for i in range(len(matrix))]\n",
    "\n",
    "evalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca4060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Words:  food, great, place, good, time, like, just, service, really, ve, delicious, best, friendly, nice, got, chicken, love, definitely, staff, hair, did, recommend, work, store, coffee, ordered, don, amazing, new, little, menu, bar, came, try, philly, restaurant, experience, shop, day, cheese, people, went, make, fresh, didn, come, know, salon, going, beer \n",
      "\n",
      "Negative Words:  order, pizza, said, told, customer, location, called, minutes, fries, asked, delivery, bad, manager, phone, wings, money, rude, worst, line, employees, waiting, pay, horrible, customers, chinese, wrong, terrible, doctor, drive, apartment, management, card, paid, waited, received, insurance, lady, starbucks, charge, woman, hotel, stay, cashier, employee, orders, slow, attitude, sales, charged, poor\n"
     ]
    }
   ],
   "source": [
    "#Attempt 2\n",
    "wordMap = vectorizer.vocabulary_\n",
    "wordMap = { j:i for i,j in wordMap.items()}\n",
    "\n",
    "positiveBusinesses = evalData.loc[evalData[\"stars\"] >= 4]\n",
    "negativeBusinesses = evalData.loc[evalData[\"stars\"] <= 3]\n",
    "positiveDic = {}\n",
    "negativeDic = {}\n",
    "for index in range(3000):\n",
    "    pScore = positiveBusinesses[\"vector\"].apply(lambda x : x[index]).sum()\n",
    "    nScore = negativeBusinesses[\"vector\"].apply(lambda x : x[index]).sum()\n",
    "    if (pScore > nScore):\n",
    "        positiveDic[index] = pScore;\n",
    "    else:\n",
    "        negativeDic[index] = nScore;\n",
    "        \n",
    "positiveDic = [wordMap[i[0]] for i in sorted(positiveDic.items(), key=lambda x:x[1])]\n",
    "negativeDic = [wordMap[i[0]] for i in sorted(negativeDic.items(), key=lambda x:x[1])]\n",
    "positiveDic.reverse()\n",
    "negativeDic.reverse()\n",
    "\n",
    "print(\"Positive Words: \", \", \".join(positiveDic[:50]), \"\\n\")\n",
    "print(\"Negative Words: \", \", \".join(negativeDic[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc00ad0",
   "metadata": {},
   "source": [
    "# Results\n",
    "Once again, we see a similar pattern. Several noise words such as \"food\" or \"order\", but the appearance of more targeted words is apparent. This is progress. I realise that perhaps avoiding the noise words is not something that can be solved exactly, but rather something that should be pushed aside.\n",
    "- Noise words exist on the classification line that seperate Positive and Negative reviews. They should exist on both dictionaries, but regardless are not important.\n",
    "- Including the 3 Starred Reviews seemed to give better results, but still in previous tests, I noticed them leaking some negative words in the positive dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529addc",
   "metadata": {},
   "source": [
    "# Attempt 3: Introducing Importance\n",
    "The factor we seem to miss. Importance. By introducing a classification algorithm (logistic regression), and training it on our data, we can extract the importance of each feature. This way, we can showcase the most high valued words of each dictionary first.\n",
    "\n",
    "Additionally, after some real world pondering, I realised that the mathematical approach on classifying our data was not entirely correct. If someone was to see a restaurant with an average of 3 stars, their reaction will most likely be to avoid it. Therefore we end up with this:\n",
    "\n",
    "We once more vectorize our data, and train a logistic regression model on them. We then, using the model's Coefficient, extract the most important words (both negative and positive). And finally, we seperate them with the same algorithm as the previous attempt, but with one difference: Negative Reviews have 1, 2 and 3 stars, while positive have 4 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ff444c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 3\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=3_000, stop_words=\"english\")\n",
    "matrix = vectorizer.fit_transform(evalData[\"text\"]).toarray()\n",
    "evalData[\"vector\"] = [matrix[i].tolist() for i in range(len(matrix))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b913001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg = sklearn.linear_model.LogisticRegression(max_iter=200)\n",
    "logReg.fit(list(evalData[\"vector\"]), evalData[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2843ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalData[\"predict\"] = logReg.predict(list(evalData[\"vector\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7c4c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Valued Words: \n",
      " great, amazing, best, delicious, great, worst, definitely, friendly, recommend, highly, super, bad, order, gem, love, good, perfect, terrible, love, horrible, delicious, reasonable, pretty, horrible, professional, philly, manager, favorite, rude, wonderful, company, excellent, friendly, drive, good, worst, overpriced, best, knowledgeable, told, definitely, rude, awesome, fantastic, honest, money, easy, don, money, incredible \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordMap = vectorizer.vocabulary_\n",
    "wordMap = { j:i for i,j in wordMap.items()}\n",
    "\n",
    "values = logReg.coef_\n",
    "flatValues = np.sort(values.flatten())\n",
    "\n",
    "highestWords = []\n",
    "highestIndexes = []\n",
    "highestValues = np.flip(flatValues) ##\n",
    "for v in highestValues:\n",
    "    i,j = np.where(np.isclose(values, v))\n",
    "    i, j = i[0], j[0]\n",
    "    highestWords.append(wordMap[j])\n",
    "    highestIndexes.append(j)\n",
    "\n",
    "print(\"Highest Valued Words: \\n\", \", \".join(highestWords[:50]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b89b4992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Words:  great, amazing, best, delicious, great, definitely, friendly, recommend, highly, super, gem, love, good, perfect, love, delicious, reasonable, pretty, professional, philly, favorite, wonderful, company, excellent, friendly, good, best, knowledgeable, definitely, awesome, fantastic, honest, easy, don, incredible, decent, awesome, beautiful, ok, helpful, nice, food, thank, fun, happy, helpful, atmosphere, fresh, unique, reviews \n",
      "\n",
      "Negative Words:  worst, bad, order, terrible, horrible, horrible, manager, rude, drive, worst, overpriced, told, rude, money, money, dirty, avoid, awful, management, told, paid, lady, told, said, manager, order, poor, customers, shut, said, employees, disgusting, nasty, employees, said, chinese, slow, pay, called, line, disgusting, asked, gross, worse, bad, attitude, mediocre, charged, asked, unprofessional\n"
     ]
    }
   ],
   "source": [
    "#Interstingly, the 3 star reviews contain the word \"bad\". Therefore, 3-star reviews are the Bad threshold o_o\n",
    "positiveBusinesses = evalData.loc[evalData[\"stars\"] >= 4]\n",
    "negativeBusinesses = evalData.loc[evalData[\"stars\"] <= 3]\n",
    "tags = []\n",
    "for index in highestIndexes:\n",
    "    pScore = positiveBusinesses[\"vector\"].apply(lambda x : x[index]).sum()\n",
    "    nScore = negativeBusinesses[\"vector\"].apply(lambda x : x[index]).sum()\n",
    "    \n",
    "    if (pScore > nScore):\n",
    "        tags.append(\"P\")\n",
    "    else:\n",
    "        tags.append(\"N\")\n",
    "        \n",
    "positiveWords = []\n",
    "negativeWords = []\n",
    "for i,j in zip(tags, highestWords):\n",
    "    if i == \"P\":\n",
    "        positiveWords.append(j)\n",
    "    else:\n",
    "        negativeWords.append(j)\n",
    "\n",
    "print(\"Positive Words: \", \", \".join(positiveWords[:50]), \"\\n\")\n",
    "print(\"Negative Words: \", \", \".join(negativeWords[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a4987",
   "metadata": {},
   "source": [
    "# Results\n",
    "This time, we get some satisfying results! Each dictionary has distinct and powerful words for its category. If I were to make some notes however on this entire method, that would be that it's limited. The number of words I can place in both categories is limited to the number of features that my vectorizer has created. Of course, there is no way to make an infinite array of positive or negative words, but in theory I can classify EVERY word on the reviews I have.\n",
    "\n",
    "# Theorizing\n",
    "A more generalized approach, or rather continuation, would be to involve some form of vectorized classification to introduce more words. The logistic regression model, or even better; a neural network, could learn from the large sample of positive/negative words, and learn to classify them! With this, we are not limited by the vectorizer, but instead are limited by our data and time.\n",
    "\n",
    "The only problem, and the main reason I did not make this, is the mere idea of feeding it every word from every review. Too laborious to be worth a preview. The results should be similar if not better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1969a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
